{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "1f889a8d-6da3-4803-be70-2e04f7502fef",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d285729c-a4fe-4d72-a997-b9749feeafa1",
   "metadata": {
    "name": "cell4",
    "collapsed": false,
    "resultHeight": 256
   },
   "source": "# Build a Retrieval Augmented Generation (RAG) based LLM assistant using Streamlit and Snowflake Cortex Search\n\n*NOTE: For prerequisites and other instructions, please refer to the [QuickStart Guide](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#0).*"
  },
  {
   "cell_type": "markdown",
   "id": "eb88841d-dec0-4785-b730-11c337d33a48",
   "metadata": {
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## Setup\n\nCreate a database and a schema."
  },
  {
   "cell_type": "code",
   "id": "fed15752-0547-4bc4-982f-d1897ecd6072",
   "metadata": {
    "language": "sql",
    "name": "Create_DB",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE DATABASE If NOT EXISTS Finance_CORTEX_SEARCH_DOCS;\nCREATE SCHEMA If NOT EXISTS DATA;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22126130-bcda-458f-a683-48bdd5a59798",
   "metadata": {
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 300
   },
   "source": "## Organize Documents and Create Pre-Processing Function\n\nStep 1. Download sample [PDF documents](https://github.com/Snowflake-Labs/sfguide-ask-questions-to-your-documents-using-rag-with-snowflake-cortex-search/tree/main).\n\nStep 2. Create a table function that will read the PDF documents and split them in chunks. We will be using the PyPDF2 and Langchain Python libraries to accomplish the necessary document processing tasks. Because as part of Snowpark Python these are available inside the integrated Anaconda repository, there are no manual installs or Python environment and dependency management required."
  },
  {
   "cell_type": "code",
   "id": "50beb326-16d5-4fa7-8d6d-129bb97637af",
   "metadata": {
    "language": "sql",
    "name": "text_chunker_UDTF",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "create or replace function text_chunker(pdf_text string)\nreturns table (chunk varchar)\nlanguage python\nruntime_version = '3.9'\nhandler = 'text_chunker'\npackages = ('snowflake-snowpark-python', 'langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport pandas as pd\n\nclass text_chunker:\n\n    def process(self, pdf_text: str):\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 1512, #Adjust this as you see fit\n            chunk_overlap  = 256, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(pdf_text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b2b9636-185d-4173-950a-4fffe04d9b24",
   "metadata": {
    "name": "cell7",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Step 3. Create a Stage with Directory Table where you will be uploading your documents."
  },
  {
   "cell_type": "code",
   "id": "8fca7c02-7e10-49e8-b46f-1091800b9bb7",
   "metadata": {
    "language": "sql",
    "name": "Create_STAGE",
    "collapsed": false,
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d19eb0-604d-4c1a-99ec-2fb850465869",
   "metadata": {
    "name": "cell9",
    "collapsed": false,
    "resultHeight": 252
   },
   "source": "Step 4. Upload documents to your staging area\n\n- Select Data on the left\n- Click on your database CC_QUICKSTART_CORTEX_SEARCH_DOCS\n- Click on your schema DATA\n- Click on Stages and select DOCS\n- On the top right click on the **+Files** botton\n- Drag and drop the PDF documents you downloaded"
  },
  {
   "cell_type": "markdown",
   "id": "090e60b9-7165-46a8-b1dd-6a0ac0a2a88a",
   "metadata": {
    "name": "cell10",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Step 5. Check files has been successfully uploaded"
  },
  {
   "cell_type": "code",
   "id": "30a46f5f-2982-477e-b5d5-9d409e66aae3",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "USE DATABASE Finance_CORTEX_SEARCH_DOCS;\nUSE SCHEMA DATA;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "982036d3-7162-4e27-bba7-4f4a13369e22",
   "metadata": {
    "language": "sql",
    "name": "ls_docs",
    "collapsed": false,
    "resultHeight": 217,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc8317e0-97cb-4bee-8316-23adea17fb68",
   "metadata": {
    "name": "cell12",
    "collapsed": false,
    "resultHeight": 127
   },
   "source": "## Pre-process and Label Documents\n\nStep 1. Create the table where we are going to store the chunks for each PDF."
  },
  {
   "cell_type": "code",
   "id": "e78a8379-81a1-4823-ac85-4b275a82e126",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "DROP TABLE IF EXISTS \nDOCS_CHUNKS_TABLE;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc188f35-e379-4dff-86f3-54a7355a3147",
   "metadata": {
    "language": "sql",
    "name": "create_table",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "-- create or replace TABLE DOCS_CHUNKS_TABLE ( \n--     RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n--     SIZE NUMBER(38,0), -- Size of the PDF\n--     FILE_URL VARCHAR(16777216), -- URL for the PDF\n--     SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n--     CHUNK VARCHAR(16777216), -- Piece of text\n--     CATEGORY VARCHAR(16777216) -- Will hold the document category to enable filtering\n-- );\n\ncreate or replace TABLE DOCS_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n    SIZE NUMBER(38,0), -- Size of the PDF\n    FILE_URL VARCHAR(16777216), -- URL for the PDF\n    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n    CHUNK VARCHAR(16777216), -- Piece of text\n    CATEGORY VARCHAR(16777216) -- Will hold the document category to enable filtering\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13b12e31-8ae9-4d23-95ae-a5d59bf27713",
   "metadata": {
    "language": "sql",
    "name": "cell13",
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "USE DATABASE Finance_CORTEX_SEARCH_DOCS;\nUSE SCHEMA DATA;\n\nSELECT * FROM DOCS_CHUNKS_TABLE;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d2a50d6-4e45-474d-9761-7bb8d5055485",
   "metadata": {
    "name": "cell14",
    "collapsed": false,
    "resultHeight": 143
   },
   "source": "Step 2. Use the CORTREX PARSE_DOCUMENT function in order to read the PDF documents from the staging area. Use the function previously created to split the text into chunks. There is no need to create embeddings as that will be managed automatically by Cortex Search service later."
  },
  {
   "cell_type": "code",
   "id": "776b5a3c-d621-4392-b331-28bb53831a68",
   "metadata": {
    "language": "sql",
    "name": "cell15",
    "resultHeight": 112
   },
   "outputs": [],
   "source": "SHOW FUNCTIONS LIKE 'TEXT_CHUNKER';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8102ebb4-08d4-4e27-a134-90c4d1d99aab",
   "metadata": {
    "language": "sql",
    "name": "insert_text_in_table",
    "collapsed": false,
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": " insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk)\n\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk\n    from \n        directory(@docs),\n        TABLE(text_chunker (TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, relative_path, {'mode': 'LAYOUT'})))) as func;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17c6b123-c948-413a-a166-632ab93ca60b",
   "metadata": {
    "name": "cell16",
    "collapsed": false,
    "resultHeight": 557
   },
   "source": "### Label the product category\n\nWe are going to use the power of Large Language Models to easily classify the documents we are ingesting in our RAG application. We are just going to use the file name but you could also use some of the content of the doc itself. Depending on your use case you may want to use different approaches. We are going to use a foundation LLM but you could even fine-tune your own LLM for your use case.\n\nFirst we will create a temporary table with each unique file name and we will be passing that file name to one LLM using Cortex Complete function with a prompt to classify what that use guide refres too. The prompt will be as simple as this but you can try to customize it depending on your use case and documents. Classification is not mandatory for Cortex Search but we want to use it here to also demo hybrid search.\n\nThis will be the prompt where we are adding the file name `Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || relative_path || '</file>'`"
  },
  {
   "cell_type": "markdown",
   "id": "e04668d3-ace4-4fa7-8ede-847c57f8c6fb",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "1fc6189b-88fa-445a-aaff-c8c958b94c3e",
   "metadata": {
    "language": "sql",
    "name": "find_categories",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "-- CREATE\n-- OR REPLACE TEMPORARY TABLE docs_categories AS WITH unique_documents AS (\n--   SELECT\n--     DISTINCT relative_path\n--   FROM\n--     docs_chunks_table\n-- ),\n-- docs_category_cte AS (\n--   SELECT\n--     relative_path,\n--     TRIM(snowflake.cortex.COMPLETE (\n--       'llama3-70b',\n--       'Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || relative_path || '</file>'\n--     ), '\\n') AS category\n--   FROM\n--     unique_documents\n-- )\n-- SELECT\n--   *\n-- FROM\n--   docs_category_cte;\n\nCREATE OR REPLACE TEMPORARY TABLE docs_categories AS\nWITH unique_documents AS (\n  -- Step 1: Extract distinct relative paths\n  SELECT DISTINCT\n    relative_path\n  FROM\n    docs_chunks_table\n),\ncategorized_documents AS (\n  -- Step 2: Categorize each unique document into one of the four companies\n  SELECT\n    relative_path,\n    TRIM(\n      snowflake.cortex.COMPLETE(\n        'llama3-70b',\n        'Given the name of the file between <file> and </file>, determine if it is related to Google, Meta, Tesla, or Microsoft. Use only one word: <file>' || relative_path || '</file>'\n      )\n    ) AS company\n  FROM\n    unique_documents\n),\nfinal_docs_with_categories AS (\n  -- Step 3: Join categorized documents back with the full docs_chunks_table\n  SELECT\n    dct.*, \n    COALESCE(cd.company, 'Unknown') AS company\n  FROM\n    docs_chunks_table dct\n  LEFT JOIN\n    categorized_documents cd\n  ON\n    dct.relative_path = cd.relative_path\n)\n-- Step 4: Create the final table\nSELECT\n  *\nFROM\n  final_docs_with_categories;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b3762b4-6338-4f22-aba2-6f29c494694b",
   "metadata": {
    "name": "cell18",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "You can check that table to identify how many categories have been created and if they are correct:"
  },
  {
   "cell_type": "code",
   "id": "203ffad6-1923-4418-8de2-043e7f2ede2e",
   "metadata": {
    "language": "sql",
    "name": "cell19",
    "collapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "-- select category from docs_categories group by category;\nselect * from docs_categories",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fbfe778-a7d6-4224-b10f-8d9452d0d8b5",
   "metadata": {
    "name": "cell20",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "We can also check that each document category is correct:"
  },
  {
   "cell_type": "markdown",
   "id": "2cf54499-bc22-4524-a834-f676d0518181",
   "metadata": {
    "name": "cell22",
    "collapsed": false,
    "resultHeight": 92
   },
   "source": "Now we can just update the table with the chunks of text that will be used by Cortex Search service to include the category for each document:"
  },
  {
   "cell_type": "code",
   "id": "e4a3270f-6d30-4327-95e1-ac80d9f5cd84",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "resultHeight": 439
   },
   "outputs": [],
   "source": "select * from docs_categories",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "676bfda9-7fcd-41b0-99ae-6f6852c8651b",
   "metadata": {
    "language": "sql",
    "name": "update_categories",
    "collapsed": false,
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- update docs_chunks_table \n--   SET category = docs_categories.category\n--   from docs_categories\n--   where  docs_chunks_table.relative_path = docs_categories.relative_path;\n\n  update docs_chunks_table \n  SET category = docs_categories.COMPANY\n  from docs_categories\n  where  docs_chunks_table.relative_path = docs_categories.relative_path;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a184c76-1b33-435c-8ecf-a7afff307422",
   "metadata": {
    "name": "cell24",
    "collapsed": false,
    "resultHeight": 415
   },
   "source": "## Create Cortex Search Service\n\nNext step is to create the CORTEX SEARCH SERVICE in the table we created before.\n\n- The name of the service is CC_SEARCH_SERVICE_CS.\n- The service will use the column chunk to create embeddings and perform retrieval based on similarity search.\n- The column category could be used as a filter.\n- To keep this service updated, warehosue COMPUTE_WH will be used. NOTE: You may replace the warehouse name with another one that you have access to.\n- The service will be refreshed every minute.\n- The data retrieved will contain the chunk, relative_path, file_url and category."
  },
  {
   "cell_type": "code",
   "id": "6971c036-f7b6-4ab3-935e-39095b44849d",
   "metadata": {
    "language": "sql",
    "name": "create_cortex_search_service",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "-- create or replace CORTEX SEARCH SERVICE CC_SEARCH_SERVICE_CS\n-- ON chunk\n-- ATTRIBUTES category\n-- warehouse = COMPUTE_WH\n-- TARGET_LAG = '1 minute'\n-- as (\n--     select chunk,\n--         relative_path,\n--         file_url,\n--         category\n--     from docs_chunks_table\n-- );\n\ncreate or replace CORTEX SEARCH SERVICE FinancialBot_SEARCH_SERVICE_CS\nON chunk\nATTRIBUTES category\nwarehouse = COMPUTE_WH\nTARGET_LAG = '1 minute'\nas (\n    select chunk,\n        relative_path,\n        file_url,\n        category\n    from docs_chunks_table\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20936e44-2a06-4117-894b-192240b84508",
   "metadata": {
    "language": "sql",
    "name": "cell23",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM docs_chunks_table LIMIT 10;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afea4572-33e5-43fa-a94b-c7a2cc72c8b3",
   "metadata": {
    "name": "cell27",
    "collapsed": false,
    "resultHeight": 127
   },
   "source": "## Build Chat Interface\n\nTo build and run chat interface in Streamlit, continue and complete the steps outlined in the [QuickStart Guide](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#4).\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "36addc51-a1bd-4f2e-bb59-252ecb286819",
   "metadata": {
    "name": "cell1"
   },
   "source": ""
  }
 ]
}